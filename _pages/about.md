---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Master Student in Fudan University. 
Focus on Speech Synthesis, Large Language Models and Multimodal Learning, and wild cat ğŸ±.

I want to build the controllable TTS/Omni Speech System ğŸ’¬, both controllability in speech prosody and speech content.
- ğŸ™ï¸ Controllable speech prosody: 
  - Richful and fine-grained controllability in speech prosody. 
  - e.g. Paralanguage, Emotion ğŸ˜„, emphasis, Timbre.
- ğŸ” Controllable speech content:
  - Controllable speech content generation.
  - e.g. Safety, Prejudice Elimination, Jailbreaking Defense ğŸ”¨.


# ğŸ”¥ News
- ğŸ‡¸ğŸ‡¬ [8/11/2025] Our 1 paper about Multimodal Understanding has been accepted by AAAI'25. ^_^
- ğŸ‡¦ğŸ‡· [11/10/2025] Have been selected to receive a free ticket (1 of 10 recipients) to Devconnect 2025 (Nov. 17â€“22, Buenos Aires, Argentina), courtesy of ETHPanda. Let's meet in Argentina!
- ğŸ‡²ğŸ‡¾ [1/10/2025] Our 1 paper about Voice Conversion has been accepted by ACM Multimedia Asia'25. ^_^
- ğŸ‡°ğŸ‡· [26/9/2025] Our 1 paper about Static Taint Analysis with Large Language Models has been accepted by ASE'25. ^_^
- ğŸ‡¨ğŸ‡³ [20/8/2025] Our 1 paper about Voice Timbre Attribute Detection has been accepted by NCMMSC'25. ^_^


# ğŸ“ Publications 

<!-- è®ºæ–‡ 6 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/ClipGlasses.png' alt="ClipGlasses" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[Not Just Whatâ€™s There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning]()

J. Xiao, **Z. Wu**, H. Lin, Y. Liu, X. Zhao, Z. Wang, Z. He, Y. Chen. *AAAI'25, CCF-A*.
</div>
</div>

<!-- è®ºæ–‡ 5 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Multimedia Asia 2025</div><img src='images/mmasia.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features](https://arxiv.org/abs/2511.10112)

W. Wang, Z. Hu, Y. Zhou, J. Xu, **Z. Wu**, C. Li and S. Li. *Multimedia Asia'25, CCF-C*.
</div>
</div>


<!-- è®ºæ–‡ 4 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NCMMSC 2025</div><img src='images/vTAD.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[QvTAD: Differential Relative Attribute Learning for Voice Timbre Attribute Detection](https://arxiv.org/abs/2508.15931)

**Z. Wu**, J. Fang, Y. Tang, Y. Zheng, Y. Wang, and H. Fei. *NCMMSC'25, CCF-N*.
- Our solution for [Voice Timbre Attribute Detection Challenge 2025](https://vtad2025-challenge.github.io).
- We build a vTAD model tailored for modeling Relative Attribute Learning.
</div>
</div>


<!-- è®ºæ–‡ 3 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CogSci 2025</div><img src='images/gibrnet.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[GIBRNet: A Multimodal Spatiotemporal Reasoning Network Integrating Emotion, Gaze, and Position for Gaze Interaction Behavior Recognition](https://escholarship.org/uc/item/9hs6x3hs)

J. Xiao, J. Gao, Yi Chen, J. Zhong, **Z. Wu**. *CogSci'25, CCF-B*.
</div>
</div>

<!-- è®ºæ–‡ 2 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='images/RecNet.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[RecNet: Optimization for Dense Object Detection in Retail Scenarios Based on View Rectification](https://ieeexplore.ieee.org/document/10887773)  

J. Xiao, Y. Chen, X. Feng, R. Wang, **Z. Wu**. *ICASSP'25, CCF-B*.
</div>
</div>


<!-- è®ºæ–‡ 1.5 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ASE 2025</div><img src='images/ase.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[Exploring Static Taint Analysis in LLMs:A Dynamic Benchmarking Framework for Measurement and Enhancement]()

H. Zhao, L. Zhang, K. Lian, F. Sun, B. Chen, Y. Liu, **Z. Wu**, Y. Zhang, M. Yang. *ASE'25, CCF-A*.
</div>
</div>

<!-- è®ºæ–‡ 1 -->

<div class='paper-box'><div class='paper-box-image style=display: flex; justify-content: center; align-items: center;'><div><div class="badge">CSCWD 2024</div><img src='images/Qiao.png' alt="Qiao" style="width:100%; height: 150px"></div></div>
<div class='paper-box-text' markdown="1">
[Qiao: DIY your routing protocol in Internet-of-Things](https://ieeexplore.ieee.org/abstract/document/10580573)  

**Z. Wu**, Y. Wang. *CSCWD'24, CCF-C*.
- We build a software router upon Linux via Golang for protocol modification and routing.
</div>
</div>


# ğŸ“– Educations
- *2024.09 - 2027.06* , M.S. in Computer Science, Fudan University, Shanghai, China.
- *2021.09 - 2024.06*, B.S. in Computer Science, Zhejiang A&F University, Zhejiang, China. 
- *2020.10 - 2021.06*, B.S. in English Language and Literature, Zhejiang A&F University, Zhejiang, China. 


# ğŸ’» Internships
<img src="images/bytedance.png" alt="TikTok Logo" width="22"/> *2025.12 - now*, Large Language Model Intern, Tiktok, [ByteDance](https://www.bytedance.com), Shanghai, China.

<img src="images/soul.jpeg" alt="Soul Logo" width="30"/> *2025.11 - 2025.12*, SpechLLMs Multimodal Intercation Research Intern, Soul AI Lab, [Soul App](https://www.soulapp.cn), Shanghai, China.

<img src="images/qf.png" alt="Qifu Logo" width="30"/> *2025.06 - 2025.11*, SpeechLLMs Reearch Intern, Department of Large Language Model, [Qfin Holdings, Inc (360DigiTech)](https://ir.qifu.tech), Shanghai, China.

<img src="images/bl.png" alt="Bilibili Logo" width="30"/> *2024.12 - 2025.06*, Speech Synthesis Intern, Department of Artificial Intelligence Platform, [BiliBili, Inc](https://www.bilibili.com), Shanghai, China.

<!-- åœ°å›¾ç»„ä»¶ -->
<div align="center" style="margin: 30px 0;">
  <h3>ğŸŒ Visitors</h3>
  <script 
    type='text/javascript' 
    id='clustrmaps' 
    src='//cdn.clustrmaps.com/map_v2.js?cl=ffefef&w=300&t=tt&d=jDMqT4JZ2aiEOCGPz7mLLpwD_DQhwYyGESzq1B7tytU&cmo=ffa0f7&cmn=ff9d47&co=a5daff&ct=000000'>
  </script>
</div>