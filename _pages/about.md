---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Master Student in Fudan University. 
Focus on Speech Synthesis, Large Language Models and Multimodal Learning, and wild cat üê±.

I want to build the controllable TTS/Omni Speech System üí¨, both controllability in speech prosody and speech content.
- üéôÔ∏è Controllable speech prosody: 
  - Richful and fine-grained controllability in speech prosody. 
  - e.g. Paralanguage, Emotion üòÑ, emphasis, Timbre.
- üîê Controllable speech content:
  - Controllable speech content generation.
  - e.g. Safety, Prejudice Elimination, Jailbreaking Defense üî®.


# üî• News
- Nothing new here.


# üìù Publications 

<!-- ËÆ∫Êñá 4 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TBD</div><img src='images/vTAD.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[QvTAD: Differential Relative Attribute Learning for Voice Timbre Attribute Detection]()

**Zhiyu Wu**, Jingyi Fang, Yufei Tang, Yuanzhong Zheng, Yaoxuan Wang, and Haojun Fei.
- Our solution for [Voice Timbre Attribute Detection Challenge 2025](https://vtad2025-challenge.github.io).
- We build a vTAD model tailored for modeling Relative Attribute Learning.
</div>
</div>


<!-- ËÆ∫Êñá 3 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CogSci 2025</div><img src='images/gibrnet.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[GIBRNet: A Multimodal Spatiotemporal Reasoning Network Integrating Emotion, Gaze, and Position for Gaze Interaction Behavior Recognition](https://escholarship.org/uc/item/9hs6x3hs)

Junhao Xiao, Jing Gao, Yi Chen, Jingxing Zhong, **Zhiyu Wu**. *CogSci'25, CCF-B*.
</div>
</div>

<!-- ËÆ∫Êñá 2 -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='images/RecNet.png' alt="RecNet" style="width:100%;"></div></div>
<div class='paper-box-text' markdown="1">
[RecNet: Optimization for Dense Object Detection in Retail Scenarios Based on View Rectification](https://ieeexplore.ieee.org/document/10887773)  

Junhao Xiao, Yi Chen, Xiao Feng, Ruoyu Wang, **Zhiyu Wu**. *ICASSP'25, CCF-B*.
</div>
</div>

<!-- ËÆ∫Êñá 1 -->

<div class='paper-box'><div class='paper-box-image style=display: flex; justify-content: center; align-items: center;'><div><div class="badge">CSCWD 2024</div><img src='images/Qiao.png' alt="Qiao" style="width:100%; height: 150px"></div></div>
<div class='paper-box-text' markdown="1">
[Qiao: DIY your routing protocol in Internet-of-Things](https://ieeexplore.ieee.org/abstract/document/10580573)  

**Zhiyu Wu**, Yisu Wang. *CSCWD'24, CCF-C*.
- We build a software router upon Linux via Golang for protocol modification and routing.
</div>
</div>


# üìñ Educations
- *2024.9 - 2027.06* , M.S. in Computer Science, Fudan University, Shanghai, China.
- *2021.9 - 2024.06*, B.S. in Computer Science, Zhejiang A&F University, Zhejiang, China. 
- *2020.10 - 2021.06*, B.S. in English Language and Literature, Zhejiang A&F University, Zhejiang, China. 


# üíª Internships

<img src="images/qf.png" alt="Qifu Logo" width="30"/> *2025.06 - now*, Speech LLM Reearch Intern, Department of Large Language Model, [Qfin Holdings, Inc (360DigiTech)](https://ir.qifu.tech), Shanghai, China.

<img src="images/bl.png" alt="Bilibili Logo" width="30"/> *2024.12 - 2025.06*, Speech Synthesis Intern, Department of Artificial Intelligence Platform, [BiliBili, Inc](https://www.bilibili.com), Shanghai, China.